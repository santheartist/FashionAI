# Fashion AI Website - Complete Project Documentation

## üìã Table of Contents
- [Project Overview](#project-overview)
- [Architecture](#architecture)
- [Existing Models](#existing-models)
- [Technology Stack](#technology-stack)
- [Setup Instructions](#setup-instructions)
- [Training New Models](#training-new-models)
- [API Documentation](#api-documentation)
- [Frontend Features](#frontend-features)
- [Deployment](#deployment)

---

## üéØ Project Overview

This is a full-stack web application for AI-powered fashion image generation and analysis. The project includes multiple deep learning models for fashion image manipulation, generation, and evaluation.

**Live Features:**
- ‚úÖ **Autoencoder**: Image reconstruction and compression (64x64 images)
- ‚úÖ **VAE (Variational Autoencoder)**: Probabilistic image generation (64x64 images)
- üîÑ **Transformer**: Text-to-image generation (needs training)
- üîÑ **Diffusion Model**: High-quality image generation (needs training)
- ‚úÖ **Model Comparison**: Side-by-side evaluation
- ‚úÖ **GDPR Compliance**: Privacy policy and data protection
- ‚úÖ **Dark Mode**: Full UI dark mode support

---

## üèóÔ∏è Architecture

```
fashion-ai-website/
‚îú‚îÄ‚îÄ backend/                      # FastAPI Python backend
‚îÇ   ‚îú‚îÄ‚îÄ app/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ main.py              # Main FastAPI application
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ models_loader.py     # Model loading and management
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ schemas.py           # Pydantic schemas
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ evaluation.py        # Model evaluation logic
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ analyzer.py          # Image analysis
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ image_search.py      # CLIP-based search
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ models/
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ autoencoder_model.py  # ‚úÖ Trained & Working
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ vae_model.py          # ‚úÖ Trained & Working
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ transformer_model.py  # üîÑ NEEDS IMPLEMENTATION
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ diffusion_model.py    # üîÑ NEEDS IMPLEMENTATION
‚îÇ   ‚îú‚îÄ‚îÄ trained_models/          # Model weights directory
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ autoencoder_64x64.pth     # ‚úÖ Available
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ vae_fashion_64x64.pth     # ‚úÖ Available
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ transformer_model.h5      # üì¶ Uploaded (needs integration)
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ diffusion_model.pth       # ‚ùå Not trained yet
‚îÇ   ‚îú‚îÄ‚îÄ data/                    # Fashion dataset
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ fashion_images/      # Fashion-MNIST or custom dataset
‚îÇ   ‚îú‚îÄ‚îÄ requirements.txt         # Python dependencies
‚îÇ   ‚îî‚îÄ‚îÄ run_server.py           # Server startup script
‚îÇ
‚îî‚îÄ‚îÄ frontend/                    # React + Vite frontend
    ‚îú‚îÄ‚îÄ src/
    ‚îÇ   ‚îú‚îÄ‚îÄ App.jsx              # Main app with routing
    ‚îÇ   ‚îú‚îÄ‚îÄ api.js               # Backend API client
    ‚îÇ   ‚îú‚îÄ‚îÄ components/          # Reusable components
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Navbar.jsx       # Navigation with dark mode
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ImageGrid.jsx    # Image display grid
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ MetricCard.jsx   # Metrics display
    ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ AutoencoderDemo.jsx
    ‚îÇ   ‚îú‚îÄ‚îÄ context/
    ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ DarkModeContext.jsx  # Dark mode state
    ‚îÇ   ‚îú‚îÄ‚îÄ pages/
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Home.jsx              # Landing page
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ VAEEvaluation.jsx     # ‚úÖ VAE interface
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ AutoencoderEvaluation.jsx  # ‚úÖ Autoencoder interface
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Evaluation.jsx        # Model evaluation
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Comparison.jsx        # Model comparison
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ModelTab.jsx          # Individual model pages
    ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ GDPR.jsx              # Privacy policy
    ‚îÇ   ‚îú‚îÄ‚îÄ index.css            # Global styles + dark mode
    ‚îÇ   ‚îî‚îÄ‚îÄ tailwind.config.js   # Tailwind configuration
    ‚îú‚îÄ‚îÄ package.json
    ‚îî‚îÄ‚îÄ vite.config.js

```

---

## ü§ñ Existing Models

### 1. ‚úÖ Autoencoder (WORKING)
**Framework:** PyTorch  
**Architecture:** Convolutional Autoencoder  
**File:** `backend/app/models/autoencoder_model.py`  
**Weights:** `backend/trained_models/autoencoder_64x64.pth`  
**Input:** 64x64 RGB images  
**Output:** 64x64 reconstructed images  

**Architecture Details:**
```python
Encoder:
  - Conv2d(3, 32, 3, 2, 1) + ReLU
  - Conv2d(32, 64, 3, 2, 1) + ReLU
  - Conv2d(64, 128, 3, 2, 1) + ReLU
  - Flatten + Linear(128*8*8, 256) -> Latent Space

Decoder:
  - Linear(256, 128*8*8) + ReLU
  - Unflatten to (128, 8, 8)
  - ConvTranspose2d(128, 64, 3, 2, 1) + ReLU
  - ConvTranspose2d(64, 32, 3, 2, 1) + ReLU
  - ConvTranspose2d(32, 3, 3, 2, 1) + Sigmoid
```

**Training Info:**
- Dataset: Fashion-MNIST or custom fashion images
- Image size: 64x64 pixels
- Normalization: Mean [0.5, 0.5, 0.5], Std [0.5, 0.5, 0.5]
- Loss: MSE (Mean Squared Error)
- Device: CPU (can use CUDA if available)

---

### 2. ‚úÖ VAE - Variational Autoencoder (WORKING)
**Framework:** PyTorch  
**Architecture:** Convolutional VAE with reparameterization trick  
**File:** `backend/app/models/vae_model.py`  
**Weights:** `backend/trained_models/vae_fashion_64x64.pth`  
**Input:** 64x64 RGB images  
**Output:** 64x64 generated images  
**Latent Dimension:** 512  

**Architecture Details:**
```python
Encoder:
  - Conv2d(3, 64, 4, 2, 1) + ReLU
  - Conv2d(64, 128, 4, 2, 1) + BatchNorm2d + ReLU
  - Conv2d(128, 256, 4, 2, 1) + BatchNorm2d + ReLU
  - Flatten
  - fc_mu: Linear(256*8*8, 512) -> Mean
  - fc_logvar: Linear(256*8*8, 512) -> Log Variance

Decoder:
  - Linear(512, 256*8*8) + ReLU
  - Unflatten to (256, 8, 8)
  - ConvTranspose2d(256, 128, 4, 2, 1) + BatchNorm2d + ReLU
  - ConvTranspose2d(128, 64, 4, 2, 1) + BatchNorm2d + ReLU
  - ConvTranspose2d(64, 3, 4, 2, 1) + Tanh
```

**Training Info:**
- Dataset: Fashion-MNIST or custom fashion images
- Image size: 64x64 pixels
- Normalization: Mean [0.5, 0.5, 0.5], Std [0.5, 0.5, 0.5]
- Loss: VAE loss (reconstruction + KL divergence)
- Latent space: 512 dimensions
- Output range: [-1, 1] (Tanh activation)

---

### 3. üîÑ Transformer Model (NEEDS TRAINING)
**Status:** Model file uploaded but not integrated  
**File:** `transformer_model.h5` (uploaded, needs architecture code)  
**Framework:** TensorFlow/Keras (based on .h5 extension)  
**Purpose:** Text-to-image generation for fashion items  

**‚ö†Ô∏è CRITICAL REQUIREMENTS FOR TRAINING:**

For detailed training instructions, see [TRANSFORMER_TRAINING.md](./docs/TRANSFORMER_TRAINING.md)

**Quick Summary:**
- Input: Text descriptions (e.g., "red dress", "blue jeans")
- Output: 64x64 or 128x128 RGB fashion images
- Required files after training:
  - `transformer_model.h5` (full model)
  - `transformer_architecture.json` (architecture)
  - `tokenizer.json` (vocabulary)
  - `training_config.json` (hyperparameters)

---

### 4. üîÑ Diffusion Model (NEEDS TRAINING)
**Status:** Not trained yet  
**Framework:** PyTorch (to match other models)  
**Purpose:** High-quality fashion image generation  
**Recommended:** DDPM (Denoising Diffusion Probabilistic Model)  

For detailed training instructions, see [DIFFUSION_TRAINING.md](./docs/DIFFUSION_TRAINING.md)

**Quick Summary:**
- Input: Random noise (optional text conditioning)
- Output: 128x128 RGB fashion images
- Architecture: U-Net with attention layers
- Training: 50,000+ images, 200+ epochs

---

## üíª Technology Stack

### Backend
- **Framework:** FastAPI 0.104.1
- **ML Frameworks:** 
  - PyTorch 2.0+ (Autoencoder, VAE, Diffusion)
  - TensorFlow/Keras (Transformer)
- **Image Processing:** Pillow, torchvision
- **Other:** OpenCLIP, numpy, pydantic

### Frontend
- **Framework:** React 18 + Vite
- **Styling:** Tailwind CSS 3.x
- **UI Libraries:** 
  - framer-motion (animations)
  - lucide-react (icons)
  - react-hot-toast (notifications)
  - recharts (charts)
- **Routing:** react-router-dom
- **State:** React Context API

---

## üöÄ Setup Instructions

### Prerequisites
- Python 3.8+
- Node.js 16+
- npm or yarn
- CUDA (optional, for GPU acceleration)

### Backend Setup
```bash
cd backend

# Create virtual environment
python -m venv venv

# Activate virtual environment
# Windows:
venv\Scripts\activate
# Linux/Mac:
source venv/bin/activate

# Install dependencies
pip install -r requirements.txt

# Run server
python run_server.py
# Server runs on http://localhost:8000
```

### Frontend Setup
```bash
cd frontend

# Install dependencies
npm install

# Run development server
npm run dev
# Server runs on http://localhost:3001
```

### Access the Application
- Frontend: http://localhost:3001
- Backend API: http://localhost:8000
- API Docs: http://localhost:8000/docs

---

## üéì Training New Models

### General Guidelines for ALL Models

#### 1. Image Preprocessing Standards
```python
# ALL MODELS MUST USE THIS PREPROCESSING
from torchvision import transforms

# For PyTorch models
transform = transforms.Compose([
    transforms.Resize((img_size, img_size)),  # 64 or 128
    transforms.ToTensor(),
    transforms.Normalize(
        mean=[0.5, 0.5, 0.5],
        std=[0.5, 0.5, 0.5]
    )  # Normalize to [-1, 1]
])
```

#### 2. Files to Provide After Training

**For PyTorch Models (Diffusion):**
```python
checkpoint = {
    'model_state_dict': model.state_dict(),
    'config': {...},  # All hyperparameters
    'normalization': {'mean': [0.5, 0.5, 0.5], 'std': [0.5, 0.5, 0.5]},
    'training_info': {...}
}
torch.save(checkpoint, 'model_name.pth')
```

**For TensorFlow Models (Transformer):**
```python
# Save architecture + weights separately
with open('model_architecture.json', 'w') as f:
    f.write(model.to_json())
model.save_weights('model_weights.h5')
```

#### 3. Required Documentation

Create a `TRAINING_LOG.md` with:
- Training date and hardware
- Dataset details
- Hyperparameters
- Training results (loss curves)
- Model architecture code
- Usage examples

See [docs/TRAINING_TEMPLATE.md](./docs/TRAINING_TEMPLATE.md) for template.

---

## üì° API Documentation

### Key Endpoints

```http
# Health check
GET /health

# List models
GET /models

# Load specific model
POST /models/{model_name}/load

# Generate with VAE
POST /models/vae/generate
{
  "prompt": "red dress",
  "num_samples": 4
}

# Reconstruct with Autoencoder
POST /models/autoencoder/reconstruct
{
  "image_path": "path/to/image"
}

# Evaluate model
POST /evaluate/{model_name}
{
  "num_samples": 20
}
```

Full API documentation: http://localhost:8000/docs

---

## üé® Frontend Features

### Current Pages

1. **Home** (`/`) - Landing page with model showcase
2. **VAE Metrics** (`/vae-evaluation`) - VAE generation interface
3. **Autoencoder Metrics** (`/autoencoder-evaluation`) - Reconstruction demo
4. **Evaluation** (`/evaluation`) - Multi-model evaluation
5. **Comparison** (`/comparison`) - Side-by-side model comparison
6. **Privacy** (`/privacy`) - GDPR compliance information

### UI Features

- ‚úÖ **Dark Mode**: Full dark mode with toggle
- ‚úÖ **Responsive Design**: Mobile, tablet, desktop
- ‚úÖ **Animations**: Framer Motion
- ‚úÖ **Toast Notifications**: Real-time feedback
- ‚úÖ **Loading States**: Spinners and skeletons

---

## üîß Integration Checklist

### Transformer Model
- [ ] Train model with text-to-image architecture
- [ ] Provide architecture code + weights + tokenizer
- [ ] Create `backend/app/models/transformer_model.py`
- [ ] Update model loader
- [ ] Create frontend page
- [ ] Add to navigation

### Diffusion Model
- [ ] Train DDPM model
- [ ] Provide architecture + checkpoint
- [ ] Create `backend/app/models/diffusion_model.py`
- [ ] Update model loader
- [ ] Create frontend page
- [ ] Add to navigation

---

## üìä Model Performance

| Model | Input | Output | Latency | Quality |
|-------|-------|--------|---------|---------|
| Autoencoder | 64x64 | 64x64 | ~0.1s | SSIM: 0.89 |
| VAE | text | 64x64 | ~2.3s | FID: TBD |
| Transformer | text | TBD | TBD | TBD |
| Diffusion | - | TBD | TBD | TBD |

---

## üö¢ Deployment

### Backend
```bash
docker build -t fashion-ai-backend .
docker run -p 8000:8000 fashion-ai-backend
```

### Frontend
```bash
npm run build
# Deploy dist/ folder to Vercel/Netlify
```

---

## üéØ Quick Start for Chatbot

When training transformer or diffusion models, provide:

**Essential Files:**
- ‚úÖ Model weights (.pth or .h5)
- ‚úÖ Model architecture code (Python file)
- ‚úÖ Training config (JSON with hyperparameters)
- ‚úÖ Tokenizer/vocabulary (for text models)

**Critical Information:**
- ‚úÖ Framework (PyTorch/TensorFlow)
- ‚úÖ Input/output dimensions
- ‚úÖ Normalization parameters
- ‚úÖ Any special preprocessing

**See detailed guides:**
- [TRANSFORMER_TRAINING.md](./docs/TRANSFORMER_TRAINING.md)
- [DIFFUSION_TRAINING.md](./docs/DIFFUSION_TRAINING.md)
- [TRAINING_TEMPLATE.md](./docs/TRAINING_TEMPLATE.md)

---

## üìù License

[Add license information]

---

**Built with ‚ù§Ô∏è for Fashion AI Research**
